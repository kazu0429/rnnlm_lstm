{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48dbfea",
   "metadata": {},
   "source": [
    "## 青空文庫　データセット作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00456df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:00:53.434202Z",
     "start_time": "2023-08-12T13:00:52.314883Z"
    }
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c64648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:01:02.237889Z",
     "start_time": "2023-08-12T13:00:55.918654Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_author_id_dict(url=\"https://www.aozora.gr.jp/index_pages/person_all.html\"):\n",
    "    '''\n",
    "    機能：著者名と著者IDの辞書を取得する関数\n",
    "    引数：青空文庫の作家リストページのURL(省略可)\n",
    "    返り値：著者名と著者IDの辞書\n",
    "    '''\n",
    "    response = request.urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    response.close()\n",
    "\n",
    "    author_id = {}\n",
    "    elms = soup.select('li a')\n",
    "    for e in elms:\n",
    "        href = e['href']\n",
    "        idx = re.findall('person([0-9]*)\\.html.*',href)\n",
    "        author_id[e.text.replace(' ','')] = f'{int(idx[0]):0=6}'\n",
    "    return author_id\n",
    "\n",
    "author_id_dict = get_author_id_dict()\n",
    "\n",
    "import pickle\n",
    "with open('author_id_dict.pkl','wb') as f:\n",
    "    pickle.dump(author_id_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f99061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:01:07.372141Z",
     "start_time": "2023-08-12T13:01:06.875783Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_local_author_id_dict():\n",
    "    import pickle\n",
    "    with open('author_id_dict.pkl','rb') as f:\n",
    "        author_id_dict = pickle.load(f)\n",
    "    return author_id_dict\n",
    "get_local_author_id_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944d1c7",
   "metadata": {},
   "source": [
    "## 書籍データをローカル環境にダウンロード\n",
    "\n",
    "+ [公開作家リスト](https://www.aozora.gr.jp/index_pages/person_all.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f0fab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:01:26.901567Z",
     "start_time": "2023-08-12T13:01:26.792165Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_pre_processing(txt):# 本文の前処理\n",
    "    '''\n",
    "    機能：文字列(テキスト)の不要な部分(本文以外の文書や「」など)を削除する関数\n",
    "    変数：前処理したい文字列\n",
    "    返り値：処理した文字列\n",
    "    '''\n",
    "    txt = txt.split('底本')[0]\n",
    "    txt = txt.replace('｜','')\n",
    "    txt = txt.replace('／','').replace('＼','').replace('―','')\n",
    "    txt = re.sub('《.*?》','', txt)\n",
    "    txt = re.sub('［＃中見出し］.*?［＃中見出し終わり］','', txt)\n",
    "    txt = re.sub('［＃.*?］','', txt)\n",
    "    txt = txt.replace('「', '').replace('」', '').replace('『','').replace('』','')\n",
    "    txt = txt.replace('\\r','').replace('\\n','').replace('\\u3000', '')\n",
    "    txt = re.sub('([。！？])', r'\\1\\n', txt)\n",
    "    return txt\n",
    "\n",
    "def make_alltext_of_specified_author(author):\n",
    "    '''\n",
    "    機能：指定した著者名の全作品を文字列として結合し、ファイルに保存する\n",
    "    変数：著者名を名字と名前を続く形で与える ex)「夏目漱石」「太宰治」\n",
    "    返り値：結合した文字列(著者の全作品)\n",
    "    '''\n",
    "    \n",
    "    all_filename_list = glob.glob(f'/Users/akimotokazuki/cards/{author_id_dict[author]}'+r'/files/*/*.txt')\n",
    "    file_all_content = ''\n",
    "    file_stem_list = []\n",
    "    for file in all_filename_list:\n",
    "        file_id = Path(file).stem.split('_')[0]\n",
    "        with open(file,'rb') as f:\n",
    "            file_content = f.read().decode('shiftjisx0213')\n",
    "        try:\n",
    "            file_content = re.split('-{10,}',file_content)[2]\n",
    "            if  file_id in file_stem_list:\n",
    "                continue\n",
    "            file_content = text_pre_processing(file_content)\n",
    "            file_all_content += file_content\n",
    "            file_stem_list.append(file_id)\n",
    "        except Exception as e: # 例外処理。置換に失敗したファイルの内容は捨てる\n",
    "            pass\n",
    "        \n",
    "    with open(f'word2vec_data/Git{author}.txt','w') as f:\n",
    "        print(file_all_content, file = f)   \n",
    "    \n",
    "    return file_all_content\n",
    "\n",
    "def create_segmented_list_of_sentences_by_author(text_data,stopword=True): \n",
    "    '''\n",
    "    機能：文章をわかち書きし、Stopword(「の」や「が」など)を文章から省く関数\n",
    "    変数：処理したい文字列、定義したストップワード\n",
    "    返り値：処理した文字列\n",
    "    '''\n",
    "    import re\n",
    "    import MeCab as mc\n",
    "    \n",
    "    dic_neo = ' -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd' \n",
    "    wakati = mc.Tagger(\"-Owakati\"+dic_neo)\n",
    "    \n",
    "    if stopword:\n",
    "        stopwords = create_japanese_stopword()\n",
    "    else:\n",
    "        stopwords = []\n",
    "    \n",
    "    text_wakati_data = [wakati.parse(i).split() for i in text_data.split('\\n')]\n",
    "    text_data_result = []\n",
    "    \n",
    "    for s in text_wakati_data:\n",
    "        s_list = []\n",
    "        for w in s:\n",
    "            if w not in stopwords:\n",
    "                s_list.append(w)\n",
    "        text_data_result.append(s_list)\n",
    "                    \n",
    "    return text_data_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459fba0",
   "metadata": {},
   "source": [
    "## データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19deac91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:02:06.667202Z",
     "start_time": "2023-08-12T13:01:37.266451Z"
    }
   },
   "outputs": [],
   "source": [
    "# 太宰治の単語データを取得\n",
    "alltext_of_author = make_alltext_of_specified_author(\"太宰治\")\n",
    "text_data_result = create_segmented_list_of_sentences_by_author(alltext_of_author, stopword=False)\n",
    "print(len(text_data_result)) # 行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82047050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:16:01.086676Z",
     "start_time": "2023-08-12T13:16:00.876132Z"
    }
   },
   "outputs": [],
   "source": [
    "uniq_words = list(set(word for sentence in text_data_result for word in sentence))\n",
    "print(len(uniq_words)) # ユニーク単語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55a555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:20:10.780316Z",
     "start_time": "2023-08-12T13:20:10.191215Z"
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データ\n",
    "train_file_content = \"\"\n",
    "for row in text_data_result[:int(len(text_data_result)/5*3.5)]:\n",
    "        for word in row:\n",
    "            train_file_content += word + \" \"\n",
    "        train_file_content += '\\n'\n",
    "print(train_file_content[:100])\n",
    "print(len(train_file_content.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251e6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:20:12.253635Z",
     "start_time": "2023-08-12T13:20:12.121660Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータ\n",
    "test_file_content = \"\"\n",
    "for row in text_data_result[int(len(text_data_result)/5*3.5):int(len(text_data_result)/5*4.2)]:\n",
    "        for word in row:\n",
    "            test_file_content += word + \" \"\n",
    "        test_file_content += '\\n'\n",
    "print(test_file_content[:100])\n",
    "print(len(test_file_content.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece61fa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:20:14.498634Z",
     "start_time": "2023-08-12T13:20:14.350449Z"
    }
   },
   "outputs": [],
   "source": [
    "# 評価データ\n",
    "valid_file_content = \"\"\n",
    "for row in text_data_result[int(len(text_data_result)/5*4.2):]:\n",
    "        for word in row:\n",
    "            valid_file_content += word + \" \"\n",
    "        valid_file_content += '\\n'\n",
    "print(valid_file_content[:100])\n",
    "print(len(valid_file_content.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8fcb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-12T13:19:15.390949Z",
     "start_time": "2023-08-12T13:19:15.258929Z"
    }
   },
   "outputs": [],
   "source": [
    "# わかち書きテキストファイル生成\n",
    "train_file_name = 'train_dazai.txt'  \n",
    "with open(train_file_name, 'w', encoding='utf-8') as f:\n",
    "    f.write(train_file_content)\n",
    "\n",
    "test_file_name = 'test_dazai.txt'  \n",
    "with open(test_file_name, 'w', encoding='utf-8') as f:\n",
    "    f.write(test_file_content)\n",
    "    \n",
    "valid_file_name = 'valid_dazai.txt' \n",
    "with open(valid_file_name, 'w', encoding='utf-8') as f:\n",
    "    f.write(valid_file_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
